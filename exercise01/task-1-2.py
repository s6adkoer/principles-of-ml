# -*- coding: utf-8 -*-
"""Ex1.2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16ygjvHjHfM3xnaCZx64RlRFoxCV04ONZ
"""

import numpy as np
import numpy.linalg as la
import itertools as it

# Creating matrix X transpose
X_t = np.array([[1, 1, 1],[1, 1, -1],[1, -1, 1],[1, -1, -1],[-1, 1, 1],[-1, 1, -1],[-1, -1, 1],[-1, -1, -1]])
# Creating rule vectors
y_110 = np.array([1, -1, -1, -1, 1, -1, -1, 1])
y_126 = np.array([1, -1, -1, -1, -1, -1, -1, 1])

# Calculating the least squares problem
omega_110 = la.lstsq(X_t, y_110,rcond=None)[0]
omega_126 = la.lstsq(X_t, y_126,rcond=None)[0]
print(f"solution to the least squares problem with rule 110: {omega_110}")
print(f"solution to the least squares problem with rule 126: {omega_126}")

# Computing y hat for both rules
y_hat_110 = np.matmul(X_t, omega_110)
y_hat_126 = np.matmul(X_t, omega_126)
print(f"y hat of rule 110: {y_hat_110}")
print(f"y of rule 110: {y_110}")
print(f"y hat of rule 126: {y_hat_126}")
print(f"y of rule 126: {y_126}")

"""We observe nothing mentionable"""

# the function phi(x) returns phi(x) as defined on the ex. sheet. We first create all subsets of length n and then multiply to calculate the parity function
def phi(x):
  n = x.shape[0]
  out = np.array([1])
  for i in range(n):
    subsets = list(it.combinations(x, i+1))
    for subset in subsets:
      out= np.concatenate((out,[np.prod(subset)]))
  return out

#Example for x = (1, -1, 1)
x = np.array([ 1, -1, 1])
print(f"x = {x}, phi(x) = {phi(x)}")

# X_t was defined before but for readability reasons it is defined again according to the ex. sheet:
X_t = np.array([[1, 1, 1],[1, 1, -1],[1, -1, 1],[1, -1, -1],[-1, 1, 1],[-1, 1, -1],[-1, -1, 1],[-1, -1, -1]])

# Phi transposed is the matrix with phi(x_i) as its rows
Phi_t = np.array([phi(X_t[0])])
for x_i in X_t[1:]:
  Phi_t = np.vstack((Phi_t, phi(x_i)))
print(f"Phi transposed = \n {Phi_t}")

y_110 = np.array([1, -1, -1, -1, 1, -1, -1, 1])
y_126 = np.array([1, -1, -1, -1, -1, -1, -1, 1])

# Calculating the least squares problem
omega_110 = la.lstsq(Phi_t, y_110,rcond=None)[0]
omega_126 = la.lstsq(Phi_t, y_126,rcond=None)[0]
print(f"solution to the least squares problem with rule 110: {omega_110}")
print(f"solution to the least squares problem with rule 126: {omega_126}")

# Computing y hat for both rules
y_hat_110 = np.matmul(Phi_t, omega_110)
y_hat_126 = np.matmul(Phi_t, omega_126)
print(f"y hat of rule 110: {y_hat_110}")
print(f"y of rule 110: {y_110}")
print(f"y hat of rule 126: {y_hat_126}")
print(f"y of rule 126: {y_126}")

# Example how we calculate rule 126 for x = -1, -1, -1
x = np.array([-1, -1, -1])
phi_x = phi(x)
out = np.matmul(phi_x, omega_126)
# expected value = 1, but because of numerical errors we get a value slightly smaller:
print(out)

"""We "trained" our "parameters" omega with all possible inputs/"data" to archieve an output as close as possible to our rule so that we get a good output according to our rule for every input. We had to pay the price by computing a high dimensional feature map Phi. Because of this our "training" aka. the search for the least squares solution is highly complex (in contrast to the computing of the rule once the parameters have been found)"""